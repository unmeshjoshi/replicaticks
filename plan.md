# Deterministic Simulation Project - Development Plan

## Project Overview & Rationale ðŸŽ¯

This project implements a **deterministic simulation** of a distributed key-value store with quorum-based consensus. The system simulates network failures, storage delays, and other real-world conditions in a completely deterministic and reproducible manner.

### **Core Design Principles**

The entire codebase is built on these fundamental principles that drive all architectural decisions:

* **Single-Threaded Event Loop:** The system is driven by a single master thread that calls `tick()` methods in a specific order. This eliminates the need for locks and prevents race conditions. All callbacks must be executed sequentially.

* **Determinism:** The goal is a deterministic simulation. This is achieved by single-threaded execution, simulated I/O, and the use of a seeded random number generator. Every run with the same seed produces identical behavior.

*On the Name tick
The name tick is intentionally chosen to evoke the image of a clock's second hand. Each call to tick() represents one discrete, indivisible moment in a "simulation clock."

*Two Roles of tick()
Service Layer (Network, Storage): The Event Loop Driver

The tick() method on these layers is reactive. It processes I/O operations that have just completed.

Application Layer (Replica, Client): The Proactive Logic Driver

The tick() method on these components is proactive. It is used for self-initiated actions like sending a periodic heartbeat or checking for timeouts.

*How This Ensures Determinism
Our design achieves determinism by systematically eliminating the primary sources of randomness found in typical distributed applications.

*Eliminates OS Thread Scheduling Non-determinism: Our design uses a single master thread. The main simulation loop becomes the only "scheduler," dictating the exact, repeatable order of all tick() calls and subsequent callbacks, eliminating race conditions.

*Eliminates Network & I/O Non-determinism: Our SimulatedNetwork and SimulatedStorage replace the unpredictable timing of real-world I/O. They use internal, deterministic queues based on a simulated clock, ensuring events always complete at the same tick and in the same order.

*Controls Randomness with a Seed: Some simulated faults, like packet loss or I/O errors, are probabilistic. To control this, we will use a pseudo-random number generator (e.g., Java's Random class) initialized with a specific seed at the start of each test run. By using the same seed, the generator produces the exact same sequence of "random" numbers, making probabilistic events completely repeatable.

* **Asynchronous, Non-Blocking I/O:** All I/O operations (network and storage) are asynchronous and must not block the event loop. We use a custom `ListenableFuture` for this, ensuring the simulation remains deterministic and efficient.  We can not use Java CompletableFuture because it provides a blocking get method and also has methods like supplyAsync which use a separate thread pool.



## Phase 1: Foundational Models & Codecs

### âœ… **COMPLETED** 
- [x] **NetworkAddress** record (`String ipAddress`, `int port`) + port validation (1-65535)
- [x] **MessageType** enum (7 types: CLIENT_GET/SET_REQUEST/RESPONSE, INTERNAL_GET/SET_REQUEST/RESPONSE)  
- [x] **Message** record (`NetworkAddress source/destination`, `MessageType`, `byte[] payload`) + null validation
- [x] **MessageCodec** interface (`encode(Message)`, `decode(byte[])`)
- [x] **JsonMessageCodec** implementation (Jackson-based, simplified without custom serializers)

### âœ… **BONUS: Structured Payload Types**
- [x] **GetRequest** record (`String key`) + validation
- [x] **SetRequest** record (`String key`, `byte[] value`) + validation  
- [x] **GetResponse** record (`String key`, `byte[] value`, `boolean found`) + validation
- [x] **SetResponse** record (`String key`, `boolean success`) + validation
- [x] **MessagePayloadSerializationTest** demonstrating type-safe messaging patterns

### âœ… **RECENTLY COMPLETED**
- [x] **VersionedValue** record (`byte[] value`, `long timestamp`) + null validation & proper byte[] equality
- [x] **Replica** class (basic properties: `String name`, `NetworkAddress`, `List<NetworkAddress> peers`) + tick() method
- [x] **Network** interface with `send()`, `receive()`, `tick()` methods + comprehensive documentation
- [x] **SimulatedNetwork** class with configurable delays, packet loss, deterministic behavior

**Phase 1 & 2 Complete! (2A + 2B + 2C):** 67/67 tests passing âœ… (advanced network simulation with comprehensive refactoring)

---

## Phase 2: Network Layer âœ… **COMPLETED** 

### Phase 2A: Basic Network Implementation âœ…
- [x] **Network** interface (`send()`, `receive()`, `tick()`) with comprehensive documentation
- [x] **SimulatedNetwork** implementation with:
  - [x] Constructor (config, seeded Random, configurable delays and packet loss)
  - [x] Internal packet queue with deterministic delivery timing
  - [x] `send()` method with packet loss simulation using seeded Random
  - [x] `tick()` method with detailed reactive Service Layer documentation
  - [x] Deterministic behavior with identical results for same seeds

### Phase 2B: Advanced Network Simulation âœ… **COMPLETED**
- [x] **Network Partitioning Support**:
  - [x] `partition(NetworkAddress source, NetworkAddress destination)` - bidirectional partition
  - [x] `partitionOneWay(NetworkAddress source, NetworkAddress destination)` - unidirectional partition  
  - [x] `healPartition(NetworkAddress source, NetworkAddress destination)` - restore connectivity
- [x] **Per-Link Configuration**:
  - [x] `setDelay(NetworkAddress source, NetworkAddress destination, int delayTicks)` - specific link delays (implementation complete)
  - [x] `setPacketLoss(NetworkAddress source, NetworkAddress destination, double lossRate)` - per-link packet loss
- [x] **Partition Management**:
  - [x] Track partitioned links in internal state
  - [x] Apply partitions during message sending (drop partitioned messages)
  - [x] Support asymmetric network conditions (Aâ†’B works, Bâ†’A blocked)

### Phase 2C: Advanced Refactoring & Code Quality âœ… **COMPLETED**
- [x] **Domain-Driven Design Improvements**:
  - [x] Extracted intention-revealing methods from `send()`: `validateMessage()`, `linkFrom()`, `shouldDropPacket()`, `calculateDeliveryTick()`
  - [x] Enhanced `NetworkLink` record with domain behavior: `isPartitioned()` method
  - [x] Explicit dependencies: `deliverPendingMessagesFor(tickTime)` parameter
- [x] **Performance & Algorithm Enhancements**:
  - [x] Replaced `LinkedList` with `PriorityQueue` for message ordering by delivery time
  - [x] Implemented `Comparable<QueuedMessage>` for automatic priority ordering
  - [x] Improved efficiency from O(n) to O(log n) for message processing
- [x] **Naming & Domain Language**:
  - [x] Renamed `removePartition()` â†’ `healPartition()` (correct distributed systems terminology)
  - [x] Renamed `oneWayPartition()` â†’ `partitionOneWay()` (natural language flow)
  - [x] Renamed `isPacketLost()` â†’ `shouldDropPacket()` (intention-revealing decision method)

### Phase 2D: Production NIO Network Implementation âœ… **COMPLETED**
- [x] **NioNetwork** production-ready implementation with Java NIO:
  - [x] Non-blocking I/O using NIO channels and Selector
  - [x] Multiple server socket binding for different addresses  
  - [x] Deterministic tick() behavior for simulation compatibility
  - [x] Network partitioning support for testing distributed scenarios
  - [x] Thread-safe concurrent access with proper resource management

- [x] **ðŸ”§ CRITICAL BUG FIXES COMPLETED**:
  - [x] **Message Queuing System**: Messages now properly queued when connections aren't ready (instead of being dropped)
  - [x] **Channel Management**: Fixed multiple channel creation issue by using `isOpen()` instead of `isConnected()` 
  - [x] **Multiple Message Handling**: Fixed TCP message framing to decode all messages in a single buffer (not just the first)
  - [x] **Test Isolation**: Fixed flaky integration tests by improving state cleanup between test runs

- [x] **Production Features**:
  - [x] Async connection establishment with proper state management
  - [x] Partial write handling with selector-based event management  
  - [x] Message queuing for connection-pending scenarios
  - [x] Comprehensive error handling and resource cleanup
  - [x] Debug logging for troubleshooting complex distributed scenarios

- [x] **Integration Testing**:
  - [x] `SimpleNioIntegrationTest`: 3 tests for basic NIO messaging scenarios
  - [x] `QuorumBasedReplicaWithRocksDbTest`: integration tests for quorum consensus using persistent RocksDB storage  
  - [x] **All NIO tests now passing**: Fixed the "1 out of 3 messages" issue and flaky behavior
  - [x] **End-to-End Production Stack**: Full NIO + RocksDB + Quorum consensus working reliably

---

## Phase 3: MessageBus Layer âœ… **COMPLETED**

- [x] **MessageBus** class:
  - [x] Constructor (Network, MessageCodec dependencies)
  - [x] `sendMessage(Message)` method  
  - [x] `registerHandler()/unregisterHandler()` for component registration
  - [x] `tick()` method with automatic message routing to registered handlers
  - [x] `broadcast()` method for multicast messaging
  - [x] Complete message routing and handler management system

---

## Phase 4: Storage Layer âœ… **COMPLETED**

- [x] **Storage** interface (`ListenableFuture<VersionedValue> get()`, `ListenableFuture<Boolean> set()`, `tick()`)
- [x] **BytesKey** record (wraps `byte[]` with proper equals/hashCode for Map keys + defensive copying)
- [x] **SimulatedStorage** implementation:
  - [x] Constructor (fault config, seeded Random)
  - [x] Async `get()`/`set()` methods returning futures
  - [x] `tick()` method completing queued operations + failure simulation
  - [x] PriorityQueue for efficient operation ordering by completion time
  - [x] Configurable delays and failure injection for testing

---

## Phase 5: ListenableFuture Implementation âœ… **COMPLETED**

- [x] **ListenableFuture<T>** class:
  - [x] Single-threaded safe design (no blocking/external threads)
  - [x] States: PENDING, SUCCEEDED, FAILED
  - [x] `complete(T)`, `fail(Throwable)`, `onSuccess(Consumer<T>)` methods
  - [x] `onFailure(Consumer<Throwable>)` method for error handling
  - [x] Multiple callback support with immediate execution if already completed
  - [x] Proper state transition validation and error handling

---

## Phase 6: Replica Quorum Logic âœ… **COMPLETED**

- [x] Enhance **Replica** class:
  - [x] Add Storage reference and quorum tracking (`Map<CorrelationId, QuorumState>`)
  - [x] `onMessageReceived()` router method implementing MessageHandler interface
  - [x] CLIENT_GET_REQUEST & CLIENT_SET_REQUEST handlers (coordinator role)
  - [x] INTERNAL_GET_REQUEST & INTERNAL_SET_REQUEST handlers (participant role) 
  - [x] INTERNAL_GET_RESPONSE & INTERNAL_SET_RESPONSE handlers (coordinator role)
  - [x] `tick()` method for request timeouts and cleanup
  - [x] Correlation ID generation and unique tracking across distributed operations
  - [x] Quorum calculation (majority) and response aggregation logic
  - [x] Timeout handling with configurable timeout ticks

### âœ… **MAJOR ARCHITECTURAL IMPROVEMENT: Clean Message API Separation**
- [x] **Client API Messages**: GetRequest/SetRequest/GetResponse/SetResponse (clean, no internal concerns)
- [x] **Internal API Messages**: Internal* versions with explicit `correlationId` fields for distributed tracking
- [x] **Type Safety**: Prevents mixing client-facing and internal distributed system messages
- [x] **Separation of Concerns**: Client API stays simple while internal API handles distributed complexity
- [x] **19 additional tests** ensuring comprehensive coverage of both API layers

---

## Phase 7: Client Implementation âœ… **COMPLETED**

- [x] **Client** class:
  - [x] Pending requests tracking (`Map<CorrelationId, ListenableFuture>`)
  - [x] `sendGetRequest()` and `sendSetRequest()` methods returning ListenableFuture
  - [x] `onMessageReceived()` response handler implementing MessageHandler interface
  - [x] `tick()` method for request timeouts and cleanup
  - [x] Correlation ID generation for unique request tracking
  - [x] Configurable timeout handling with async future completion
  - [x] Clean client API using GetRequest/SetRequest/GetResponse/SetResponse
  - [x] Request/response matching by key for client-side correlation
  - [x] Comprehensive error handling and timeout management

---

## Phase 8: Advanced Integration Testing âœ… **COMPLETED**

### âœ… **COMPREHENSIVE DISTRIBUTED SYSTEM SCENARIOS**
- [x] **End-to-End Operations**: Full Client â†’ Replica â†’ Storage â†’ Network simulation with quorum consensus
- [x] **Network Partition Tolerance**: Split-brain scenarios, majority/minority partition handling (CAP theorem)
- [x] **Replica Failure Handling**: Graceful degradation, automatic failover, continued operation
- [x] **Concurrent Operations**: Multiple clients, proper serialization, high-throughput scenarios
- [x] **Quorum Requirements**: W+R > N consistency model, prevents split-brain writes
- [x] **Conflict Resolution**: Last-write-wins with timestamps, automatic partition healing
- [x] **Eventual Consistency**: Convergence demonstration, anti-entropy mechanisms
- [x] **Read Repair**: Automatic stale data detection, background synchronization

### âœ… **REAL-WORLD DISTRIBUTED SCENARIOS**
- [x] **`shouldPerformEndToEndGetSetOperation()`**: Basic distributed GET/SET with 5-replica quorum
- [x] **`shouldHandleNetworkPartition()`**: Partition tolerance with 3-vs-2 replica splits
- [x] **`shouldHandleReplicaFailure()`**: Byzantine fault tolerance with replica removal
- [x] **`shouldHandleConcurrentOperations()`**: High-contention concurrent client operations
- [x] **`shouldDemonstrateQuorumRequirements()`**: Quorum consensus validation
- [x] **`shouldHandleConflictResolution()`**: Partition healing and consistency recovery
- [x] **`shouldDemonstrateEventualConsistency()`**: Convergence after network partitions
- [x] **`shouldHandleReadRepairScenario()`**: Read-time consistency repairs

### âœ… **TECHNICAL ACHIEVEMENTS**
- [x] **Enhanced Serialization**: Introduced generic payload encode/decode in `MessageCodec`; Client and Replica now reuse injected codec instead of static ObjectMapper.
- [x] **Comprehensive Test Framework**: `DistributedSystemIntegrationTest` with 8 advanced scenarios
- [x] **Deterministic Simulation**: All scenarios reproducible with consistent results
- [x] **Production-Ready Features**: Handles real-world failure scenarios comprehensively

---

## Phase 9: Clean Architecture - Common Building Blocks âœ… **COMPLETED**

### âœ… **ARCHITECTURAL INSIGHT: Separation of Common vs Algorithm-Specific Logic**
**Problem**: The original `Replica` class contained both common building blocks (shared across all replication algorithms) and quorum-specific logic (only relevant to quorum-based consensus).

**Solution**: Extracted common building blocks into a reusable architecture that supports multiple replication algorithms (Raft, Chain Replication, Byzantine Fault Tolerance, etc.).

### âœ… **ARCHITECTURAL REFACTORING COMPLETED**
- [x] **`Replica` (Abstract Base Class)**:
  - [x] Common building blocks: replica identity, request ID generation, timeout management
  - [x] Message routing dispatcher (`onMessageReceived()`)
  - [x] Storage and MessageBus integration
  - [x] Serialization/deserialization utilities
  - [x] Basic `tick()` lifecycle management
  - [x] Abstract `PendingRequest` base class for request tracking

- [x] **`QuorumBasedReplica extends Replica`**:
  - [x] Quorum-specific consensus logic (QuorumState, majority calculation)
  - [x] Coordinator/participant role handling
  - [x] Quorum response aggregation and conflict resolution
  - [x] Timestamp-based ordering for distributed consistency
  - [x] All existing quorum functionality preserved

- [x] **Clean Test Architecture**:
  - [x] `ReplicaBaseTest` - Tests common building blocks (12 tests)
  - [x] `QuorumBasedReplicaTest` - Tests quorum-specific logic (renamed from `EnhancedReplicaTest`)
  - [x] `ReplicaTest` - Tests basic abstract replica functionality

### âœ… **ARCHITECTURAL BENEFITS ACHIEVED**
- [x] **Reusability**: Common building blocks can be shared across different replication algorithms
- [x] **Maintainability**: Clear separation of concerns between infrastructure and algorithm logic
- [x] **Extensibility**: Easy to implement new replication algorithms (Raft, Chain Replication, etc.)
- [x] **Backward Compatibility**: Existing code continues to work without changes
- [x] **Type Safety**: Proper inheritance hierarchy with abstract base classes

---

## Phase 10: Critical Production Architecture Fixes âœ… **COMPLETED**

### âœ… **MAJOR FIX: Client Connection Architecture**
**Problem**: Client was registering against IP addresses upfront instead of establishing connections per request, violating real-world distributed system patterns.

**Solution**: Implemented industry-standard bootstrap discovery pattern inspired by Kafka and TigerBeetle:
- [x] **Bootstrap Replicas Pattern**: Client constructor now accepts `List<NetworkAddress> bootstrapReplicas`
- [x] **Dynamic Connection Establishment**: `establishConnectionAndSend()` method connects per request
- [x] **Network Layer Abstraction**: Added `establishConnection()` method to Network interface
- [x] **SimulatedNetwork Implementation**: Uses localhost with ephemeral ports for deterministic testing
- [x] **NioNetwork Implementation**: Real socket connections with OS-assigned addresses after blocking connect
- [x] **Failover Support**: Client iterates through bootstrap replicas for automatic failover
- [x] **Connection Pooling Ready**: Architecture supports future connection reuse optimizations

### âœ… **CRITICAL FIX: NIONetwork Bidirectional Communication**
**Problem**: NIONetwork was not properly handling client-side vs server-side connections, causing response delivery issues.

**Solution**: Implemented proper channel management following production networking patterns:
- [x] **Accepted Client Channels**: Added `acceptedClientChannels` map to track server-side connections
- [x] **Enhanced handleAccept()**: Properly stores accepted client connections with remote address keys
- [x] **Updated sendMessageDirectly()**: Checks accepted channels first for sending responses back to clients
- [x] **Extended findDestinationForChannel()**: Searches both outbound and accepted channel types
- [x] **Structural vs Behavioral Separation**: Followed TDD and Tidy First principles
- [x] **Test Validation**: All 220+ tests continue to pass after structural changes

### âœ… **KNOWLEDGE ARTIFACT: "Growing a Language" Article**
**Achievement**: Created comprehensive article demonstrating Guy Steele's "Growing a Language" concept using our development session.

**Content Includes**:
- [x] **7 Architectural Layers**: Foundation â†’ Network â†’ Storage â†’ Messaging â†’ Consensus â†’ Client â†’ Testing
- [x] **Language Evolution**: From raw Java networking to domain-specific distributed systems vocabulary
- [x] **AI as Language Growth Catalyst**: Pattern recognition, architectural guidance, refactoring direction
- [x] **Complex Algorithm Enablement**: How grown language enables LLM implementation of Paxos, Raft, PBFT
- [x] **Industry Pattern Integration**: Real-world examples from Kafka, TigerBeetle, distributed systems

### âœ… **ARCHITECTURAL IMPROVEMENTS**
- [x] **Connection Management**: Proper separation of client/server connection lifecycle
- [x] **Bootstrap Discovery**: Industry-standard cluster discovery and failover patterns
- [x] **Network Abstraction**: Clean interface supporting both simulation and production
- [x] **Production Readiness**: Real networking with proper connection establishment
- [x] **Testing Determinism**: Maintained simulation capabilities for reproducible testing

---

## Phase 11: NIONetwork Critical Client-Server Connection Fixes âœ… **COMPLETED**

### âœ… **DIRECT-CHANNEL RESPONSE SUPPORT**
- [x] Added sendOnChannel(SocketChannel, Message) to Network interface and implemented in NioNetwork
- [x] SimulatedNetwork implements sendOnChannel as a no-op fallback to send()
- [x] MessageContext infrastructure preserves channel for request-response correlation
- [x] BaseMessageBus.reply() delegates to sendOnChannel when possible
- [x] QuorumBasedReplica and all message handlers use reply() for responses
- [x] Integration tests verify direct-channel response path

## Phase 12: Centralized Timeout & Tick Management (TigerBeetle-Style) âœ… **COMPLETED**

### âœ… **TIMEOUT MANAGEMENT REFACTORING**
- [x] **Timeout Class**: Created TigerBeetle-style Timeout class with internal tick counter, start/stop/reset methods
- [x] **Client Integration**: Refactored Client to use Timeout objects for request timeouts, removing internal tick counters
- [x] **Replica Integration**: Refactored QuorumBasedReplica to use Timeout objects for request timeouts
- [x] **API Simplification**: Removed tick parameters from interfaces, components manage internal counters where needed
- [x] **Test Updates**: Updated all tests to use Timeout and match new API without tick parameters
- [x] **Deterministic Testing**: Fixed test logic to ensure timeouts are tested deterministically

### âœ… **CENTRALIZED TICK ORCHESTRATION**
- [x] **SimulationDriver**: Created centralized tick orchestration following TigerBeetle's approach
- [x] **Deterministic Order**: Components ticked in order: Clients â†’ Replicas â†’ Networks â†’ MessageBuses â†’ Storage
- [x] **MessageBus Support**: Extended SimulationDriver to include MessageBus ticking for proper message routing
- [x] **Integration Updates**: Updated DistributedSystemIntegrationTest and DirectChannelOrderTest to use SimulationDriver
- [x] **Test Utilities**: Added TestUtils.runUntil() for test scenarios that need to wait for conditions
- [x] **Redundant Ticking Removal**: Eliminated issue where Replica was calling storage.tick() internally

### âœ… **ARCHITECTURAL BENEFITS ACHIEVED**
- [x] **Single Source of Truth**: SimulationDriver is the only component responsible for tick orchestration
- [x] **TigerBeetle Alignment**: Follows same tick management pattern as reference implementation
- [x] **Deterministic Behavior**: All components progress together in predictable order
- [x] **Test Reliability**: Tests are more reliable with centralized tick management
- [x] **Clean Separation**: No component ticks another component internally

### âœ… **TEST RESULTS**
- [x] **241 tests total** - All passing âœ…
- [x] **0 failures** âœ…
- [x] **Clean build** âœ…

## Phase 13: Direct-Channel Response Architecture âœ… **COMPLETED**

### âœ… **END-TO-END TEST COVERAGE**
- [x] DirectChannelResponseTest exercises direct-channel response path for both SimulatedNetwork and NioNetwork
- [x] Multiple requests and responses on the same channel are handled correctly
- [x] Fallback for simulation works seamlessly
- [x] All tests pass, confirming correct implementation

## Phase 14: Client-Server Dual Routing Architecture âœ… **COMPLETED**

### âœ… **COMPLETED: Enhanced MessageBus with Dual Routing Patterns**

### Scope
Implemented support for both client and server routing patterns in MessageBus:
- **Server Pattern**: Register handler by address (IP:port) - for servers that listen on specific addresses
- **Client Pattern**: Register handler by correlation ID - for clients that send requests and expect responses

### **ARCHITECTURAL ANALYSIS**
**Problem Solved**: MessageBus now supports both server pattern (address-based routing) and client pattern (correlation ID-based routing), following real-world distributed system patterns.

**Solution Implemented**: Enhanced MessageBus with dual routing capabilities:
1. **Server Pattern** (existing): `messageBus.registerHandler(address, handler)`
2. **Client Pattern** (new): `messageBus.registerHandler(correlationId, handler)`

### **IMPLEMENTATION COMPLETED**

#### **Step 0: Tidy First - Correlation ID as First-Class Field** âœ…
- [x] Add `correlationId` as a required field to the `Message` record
- [x] Update all usages, constructors, and tests to include/provide a correlation ID
- [x] Update request/response classes and tests to propagate correlation IDs

#### **Step 1: Structural Changes (Tidy First)** âœ…
- [x] Add correlation ID handler map to MessageBus
- [x] Add client handler registration method
- [x] Add correlation ID registration/unregistration methods
- [x] Update MessageBus constructor and fields

#### **Step 2: Enhanced Routing Logic** âœ…
- [x] Implement dual routing in `routeMessagesToHandlers()`
- [x] Add correlation ID extraction from message (now trivial)
- [x] Prioritize correlation ID routing over address routing
- [x] Handle cleanup of one-time correlation ID handlers

#### **Step 3: Client Integration** âœ…
- [x] Update Client to register handlers by correlation ID
- [x] Remove address-based registration from Client
- [x] Update request/response matching to use correlation IDs
- [x] Add correlation ID to request/response payloads

#### **Step 4: Network Layer Support** âœ…
- [x] Add `receiveAll()` method to Network interface for client message reception
- [x] Implement in SimulatedNetwork and NioNetwork
- [x] Update MessageBus to use appropriate receive method based on pattern

#### **Step 5: Testing & Validation** âœ…
- [x] Create tests for correlation ID-based routing
- [x] Update existing tests to work with new dual routing
- [x] Verify both server and client patterns work correctly
- [x] Ensure backward compatibility

### **DESIGN DECISIONS IMPLEMENTED**
1. **Correlation ID Extraction**: Now trivial, as every Message has a correlationId field
2. **Client Message Reception**: Clients register for ALL messages, then filter by correlation ID
3. **Handler Cleanup**: Remove correlation ID handlers after one use (request-response cycle)
4. **Backward Compatibility**: Maintain existing server pattern unchanged

### **BENEFITS ACHIEVED**
- [x] **Proper Client Architecture**: Clients no longer need to register against ephemeral IP addresses
- [x] **Real-World Patterns**: Follows industry-standard request-response correlation patterns
- [x] **Clean Separation**: Clear distinction between server and client routing patterns
- [x] **Scalability**: Supports multiple concurrent client requests with proper correlation
- [x] **Maintainability**: Single MessageBus handles both patterns cleanly

### **TEST RESULTS**
- [x] **All tests passing** - 241 tests total âœ…
- [x] **DirectChannelNioTest** - Verifies direct-channel response functionality âœ…
- [x] **Integration tests** - All distributed system scenarios working âœ…
- [x] **Clean build** - No compilation errors âœ…

## Phase 15: Architecture Refactoring - Clean Separation of Client/Server Roles âœ… **COMPLETED**

### âœ… **ARCHITECTURAL REFACTORING COMPLETED**
**Problem Solved**: MessageBus and Network played dual roles (client and server), making the code confusing and potentially fragile. The routing logic was complex and the responsibilities were mixed.

**Solution Implemented**: Refactored to separate client and server components with clear responsibilities.

### âœ… **REFACTORING IMPLEMENTED**

#### **Step 1: Structural Changes (Tidy First)** âœ…
- [x] Created `BaseMessageBus` abstract class with common functionality
- [x] Created `ClientMessageBus` class for client-side message handling
- [x] Created `ServerMessageBus` class for server-side message handling  
- [x] Updated interfaces to separate client/server concerns

#### **Step 2: Component Integration** âœ…
- [x] Updated `Client` to use `ClientMessageBus` 
- [x] Updated `Replica` and `QuorumBasedReplica` to use `BaseMessageBus`
- [x] Updated `SimulationDriver` to handle separate client/server components
- [x] Updated all test files to use appropriate message bus types

#### **Step 3: Testing & Validation** âœ…
- [x] Updated all tests to use new separated components
- [x] Verified both client and server patterns work correctly
- [x] Ensured no regression in functionality
- [x] Removed legacy `MessageBus.java` completely

### âœ… **BENEFITS ACHIEVED**
- [x] **Clear Separation**: Client and server responsibilities are clearly separated
- [x] **Simplified Logic**: Each component has a single, clear responsibility
- [x] **Better Maintainability**: Easier to understand and modify each component
- [x] **Improved Testability**: Components can be tested in isolation
- [x] **Future Extensibility**: Cleaner foundation for adding new features

### âœ… **TEST RESULTS**
- [x] **All tests passing** - 241 tests total âœ…
- [x] **Clean build** - No compilation errors âœ…
- [x] **Legacy removal** - Old MessageBus completely removed âœ…

## Phase 16: Production Runner & Demo Infrastructure âœ… **COMPLETED**

### **SCOPE**
Create production-ready command-line applications and demo infrastructure for running the distributed key-value store in a real cluster environment.

### **COMPLETED COMPONENTS**

#### **Step 1: Command-Line Applications** âœ…
- [x] Created `ServerApplication` class with comprehensive argument parsing
- [x] Implemented server application that can start a replica node with configurable parameters
- [x] Created `ClientApplication` class for setting/getting values via command-line
- [x] Added command-line argument parsing and configuration (--port, --data-dir, --peers, etc.)
- [x] Implemented proper error handling and usage documentation

#### **Step 2: Demo Infrastructure** âœ…
- [x] Created `demo/run-demo.sh` script to orchestrate a 3-node cluster
- [x] Implemented cluster startup/shutdown procedures with proper cleanup
- [x] Created client demo operations for setting/getting values across the cluster
- [x] Added comprehensive logging and monitoring capabilities
- [x] Implemented proper error handling and graceful shutdown

#### **Step 3: Production Features** âœ…
- [x] Added fat JAR support with Gradle Shadow plugin for all dependencies
- [x] Implemented proper logging framework with debug output
- [x] Created deployment documentation and usage examples
- [x] Added server process management and cleanup scripts

### **ACHIEVED BENEFITS**
- [x] **Production Ready**: Real-world deployment capabilities with fat JARs
- [x] **Demo Capability**: Easy demonstration of distributed system features
- [x] **User Experience**: Simple command-line interface for users
- [x] **Documentation**: Living examples of system usage
- [x] **Cluster Management**: Automated 3-node cluster orchestration

### **TEST RESULTS**
- [x] **Demo script working**: 3-node cluster starts, performs operations, and shuts down cleanly âœ…
- [x] **Fat JARs created**: Both server and client JARs include all dependencies âœ…
- [x] **All tests passing**: 247+ tests continue to pass after production additions âœ…

---

## Phase 17: Production Client-Server Communication & Fat JAR Infrastructure âœ… **COMPLETED**

### **SCOPE**
Fix critical client-server communication issues and establish robust production infrastructure with fat JARs.

### **COMPLETED COMPONENTS**

#### **Step 1: Fat JAR Infrastructure** âœ…
- [x] **Gradle Shadow Plugin**: Added shadow plugin for creating fat JARs with all dependencies
- [x] **Server Fat JAR**: `replicated-server.jar` includes all dependencies (Jackson, RocksDB, etc.)
- [x] **Client Fat JAR**: `replicated-client.jar` includes all dependencies for standalone client operations
- [x] **Build Configuration**: Proper Gradle tasks for building both server and client JARs
- [x] **Dependency Management**: All external dependencies properly included and merged

#### **Step 2: Client-Server Communication Fixes** âœ…
- [x] **Critical Bug Fix**: Client was not calling `messageBus.tick()` during polling, causing timeouts
- [x] **Network Event Processing**: Added proper tick() calls to process outbound messages and incoming responses
- [x] **Debug Logging**: Added comprehensive debug logging to track request/response flow
- [x] **Connection Management**: Fixed client connection establishment and message routing
- [x] **Timeout Handling**: Improved timeout handling with proper network event processing

#### **Step 3: Production Demo Infrastructure** âœ…
- [x] **Updated Demo Script**: Modified `demo/run-demo.sh` to use fat JARs instead of classpath execution
- [x] **Server Process Management**: Added proper server startup, monitoring, and cleanup
- [x] **Client Operations**: Integrated real client application for SET/GET operations
- [x] **Cluster Orchestration**: Automated 3-node cluster with proper peer configuration
- [x] **Error Handling**: Comprehensive error handling and graceful shutdown procedures

#### **Step 4: Testing & Validation** âœ…
- [x] **End-to-End Testing**: Verified client-server communication works with fat JARs
- [x] **Demo Validation**: Confirmed 3-node cluster demo works correctly
- [x] **Integration Testing**: All integration tests continue to pass
- [x] **Production Readiness**: System ready for real-world deployment

### **CRITICAL FIXES IMPLEMENTED**
- [x] **Client Tick Processing**: Client now calls `messageBus.tick()` during polling to process network events
- [x] **Message Routing**: Fixed message routing between client and server components
- [x] **Connection Lifecycle**: Proper connection establishment and cleanup
- [x] **Response Handling**: Client properly receives and processes server responses
- [x] **Timeout Resolution**: Eliminated client timeout issues through proper network event processing

### **ACHIEVED BENEFITS**
- [x] **Production Deployment**: Fat JARs enable easy deployment without dependency management
- [x] **Reliable Communication**: Client-server communication works reliably in production
- [x] **Debug Capability**: Comprehensive logging for troubleshooting production issues
- [x] **User Experience**: Simple command-line tools for interacting with the distributed system
- [x] **Cluster Management**: Automated cluster orchestration for demonstrations and testing

### **TEST RESULTS**
- [x] **All tests passing**: 247+ tests continue to pass âœ…
- [x] **Demo working**: 3-node cluster demo completes successfully âœ…
- [x] **Fat JARs functional**: Both server and client JARs work correctly âœ…
- [x] **Production ready**: System ready for real-world deployment âœ…

---

## Phase 18: Architecture Naming & Documentation Cleanup âœ… **COMPLETED**

### âœ… **SCOPE**
Improve code clarity and maintainability by renaming components to better reflect their dual role in both simulation and production environments.

### âœ… **COMPLETED COMPONENTS**

#### **Step 1: Code Quality & Unused Code Removal** âœ…
- [x] **Toxicity Analysis**: Conducted comprehensive code toxicity analysis based on Erik DÃ¶rnenburg's framework
- [x] **Unused Code Removal**: Removed 3 unused classes (`SystemClock`, `TickManager`, `ClientRequestCallback`)
- [x] **Import Cleanup**: Cleaned up unused imports across all files
- [x] **Method Cleanup**: Removed unused methods and optimized existing code
- [x] **ConcurrentModificationException Fix**: Fixed critical client timeout handling bug with ConcurrentHashMap

#### **Step 2: Documentation Updates** âœ…
- [x] **Comprehensive README.md**: Created complete project documentation with usage instructions
- [x] **Architecture Documentation**: Documented the complete system architecture and design principles
- [x] **Usage Examples**: Added clear examples for running simulations and production clusters
- [x] **API Documentation**: Documented all public APIs and interfaces

### âœ… **ACHIEVED BENEFITS**
- [x] **Code Quality**: Eliminated toxic code patterns and improved maintainability
- [x] **Documentation**: Comprehensive documentation for users and contributors
- [x] **Clean Codebase**: Removed unused code and optimized existing implementations
- [x] **Bug Fixes**: Resolved critical concurrency issues in client implementation

---

## Phase 19: TigerBeetle-Style SimulationRunner âœ… **COMPLETED**

### âœ… **SCOPE**
Implement a comprehensive TigerBeetle-style SimulationRunner for continuous stress testing with failure injection capabilities.

### âœ… **COMPLETED COMPONENTS**

#### **Step 1: SimulationRunner Implementation** âœ…
- [x] **TigerBeetle-Style Design**: Created SimulationRunner following TigerBeetle's architecture patterns
- [x] **Continuous Workload Generation**: Random GET/SET operations running continuously
- [x] **Failure Injection**: Network partitions, packet loss, delays, storage failures
- [x] **Real-time Monitoring**: Live statistics collection and reporting
- [x] **Configurable Scenarios**: Multiple failure modes (mild, aggressive, chaos)

#### **Step 2: Advanced Failure Testing** âœ…
- [x] **Network Partitions**: Dynamic partition creation and healing
- [x] **Packet Loss Simulation**: Configurable packet loss rates per scenario
- [x] **Delay Injection**: Network delay simulation for testing timeout scenarios
- [x] **Storage Failures**: Configurable storage failure injection
- [x] **Combination Failures**: Multiple simultaneous failure types

#### **Step 3: Monitoring & Statistics** âœ…
- [x] **Operation Tracking**: Real-time success/failure/timeout counters
- [x] **Performance Metrics**: Throughput (operations/second) and tick rate monitoring
- [x] **Failure Statistics**: Detailed breakdown of failure types and causes
- [x] **Replica Health**: Individual replica status monitoring
- [x] **Final Summary**: Comprehensive end-of-simulation statistics

#### **Step 4: Production Testing** âœ…
- [x] **5-Minute Stress Test**: Successfully completed 5-minute continuous simulation
- [x] **Outstanding Performance**: 65,491 operations with 98.7% success rate
- [x] **High Throughput**: Sustained 218 operations/second for 5 minutes
- [x] **Excellent Resilience**: Handled 50 network failures gracefully
- [x] **Zero System Failures**: All replicas remained operational throughout

### âœ… **SIMULATION RESULTS - PRODUCTION READY**
- **Duration**: 300 seconds (5 minutes) âœ…
- **Total Operations**: 65,491 operations âœ…
- **Success Rate**: 98.7% (64,637 successful) âœ…
- **Throughput**: ~218 operations/second sustained âœ…
- **Average TPS**: 1,091 ticks/second âœ…
- **System Stability**: Zero crashes, all replicas operational âœ…

### âœ… **ACHIEVED BENEFITS**
- [x] **Production Confidence**: Demonstrated system can handle real-world load and failures
- [x] **TigerBeetle Alignment**: Follows same robust testing methodology as reference implementation
- [x] **Continuous Testing**: Capability for long-running stability testing
- [x] **Failure Resilience**: Proven ability to handle network partitions and failures
- [x] **Performance Validation**: Confirmed high-throughput distributed consensus capability

### âœ… **TEST RESULTS**
- [x] **SimulationRunner working**: Runs continuously with configurable failure injection âœ…
- [x] **All scenarios passing**: Mild, aggressive, and chaos modes all functional âœ…
- [x] **Long-duration stable**: 5+ minute runs complete successfully âœ…
- [x] **Production ready**: System demonstrated ready for real-world deployment âœ…

---

## Phase 20: Next Steps & Future Enhancements ðŸ”„ **PLANNED**

### **PLANNED COMPONENTS**

#### **Step 1: MessageCodec Architecture Improvements** âœ…
- [x] **Unified Serialization Methods**: Refactor MessageCodec to have generic encode/decode methods that work for both Message objects and arbitrary payloads, eliminating the separate encodePayload/decodePayload methods
- [x] **Direct MessageCodec Injection**: Inject MessageCodec directly into Replica and Client constructors instead of accessing it through messageBus.getMessageCodec(), improving dependency clarity and testability

#### **Step 2: Advanced Consensus Algorithms** ðŸ”„
- [ ] **Raft Consensus**: Implement Raft algorithm as alternative to quorum-based consensus
- [ ] **Byzantine Fault Tolerance**: Add PBFT (Practical Byzantine Fault Tolerance) implementation
- [ ] **Chain Replication**: Implement chain replication for high-performance scenarios
- [ ] **Algorithm Comparison**: Benchmarking framework for comparing consensus algorithms

#### **Step 3: Performance Optimizations** ðŸ”„
- [ ] **Connection Pooling**: Optimize network layer with connection reuse
- [ ] **Batch Operations**: Support for batched GET/SET operations
- [ ] **Compression**: Message compression for network bandwidth optimization
- [ ] **Zero-Copy Serialization**: Optimize serialization performance

#### **Step 4: Production Features** ðŸ”„
- [ ] **Persistence**: Add WAL (Write-Ahead Logging) for durability
- [ ] **Snapshotting**: Implement periodic state snapshots
- [ ] **Log Compaction**: Add log compaction for storage efficiency
- [ ] **Cluster Membership**: Dynamic cluster membership changes

### **EXPECTED BENEFITS**
- [ ] **Algorithm Flexibility**: Multiple consensus options for different use cases
- [ ] **Performance**: Optimized for high-throughput production workloads
- [ ] **Enterprise Features**: Production-ready features for enterprise deployment
- [ ] **Research Platform**: Foundation for distributed systems research and experimentation

---

## Project Status: PRODUCTION READY ðŸŽ¯

### âœ… **ACHIEVEMENTS SUMMARY**
- **Complete Distributed System**: Functional quorum-based key-value store
- **Production Infrastructure**: Fat JARs, command-line tools, cluster orchestration
- **Comprehensive Testing**: 247+ tests, 5-minute stress testing, failure injection
- **TigerBeetle-Style Simulation**: Continuous testing with advanced failure scenarios
- **Clean Architecture**: Well-separated concerns, extensible for multiple consensus algorithms
- **Documentation**: Complete README, architecture documentation, usage examples

### âœ… **TECHNICAL METRICS**
- **Code Quality**: Clean, well-tested, following TDD and Clean Architecture principles
- **Performance**: 218+ ops/sec sustained throughput, 98.7% success rate under stress
- **Reliability**: Zero system failures during 5-minute stress testing
- **Extensibility**: Abstract base classes support multiple replication algorithms
- **Production Ready**: Fat JARs, CLI tools, cluster management, comprehensive documentation

**This project demonstrates a complete, production-ready distributed key-value store with comprehensive testing and simulation capabilities, following industry best practices and TigerBeetle-style robustness testing.**