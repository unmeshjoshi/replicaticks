# Deterministic Simulation Project - Development Plan

## Project Overview & Rationale üéØ

This project implements a **deterministic simulation** of a distributed key-value store with quorum-based consensus. The system simulates network failures, storage delays, and other real-world conditions in a completely deterministic and reproducible manner.

### **Core Design Principles**

The entire codebase is built on these fundamental principles that drive all architectural decisions:

* **Single-Threaded Event Loop:** The system is driven by a single master thread that calls `tick()` methods in a specific order. This eliminates the need for locks and prevents race conditions. All callbacks must be executed sequentially.

* **Determinism:** The goal is a deterministic simulation. This is achieved by single-threaded execution, simulated I/O, and the use of a seeded random number generator. Every run with the same seed produces identical behavior.

*On the Name tick
The name tick is intentionally chosen to evoke the image of a clock's second hand. Each call to tick() represents one discrete, indivisible moment in a "simulation clock."

*Two Roles of tick()
Service Layer (Network, Storage): The Event Loop Driver

The tick() method on these layers is reactive. It processes I/O operations that have just completed.

Application Layer (Replica, Client): The Proactive Logic Driver

The tick() method on these components is proactive. It is used for self-initiated actions like sending a periodic heartbeat or checking for timeouts.

*How This Ensures Determinism
Our design achieves determinism by systematically eliminating the primary sources of randomness found in typical distributed applications.

*Eliminates OS Thread Scheduling Non-determinism: Our design uses a single master thread. The main simulation loop becomes the only "scheduler," dictating the exact, repeatable order of all tick() calls and subsequent callbacks, eliminating race conditions.

*Eliminates Network & I/O Non-determinism: Our SimulatedNetwork and SimulatedStorage replace the unpredictable timing of real-world I/O. They use internal, deterministic queues based on a simulated clock, ensuring events always complete at the same tick and in the same order.

*Controls Randomness with a Seed: Some simulated faults, like packet loss or I/O errors, are probabilistic. To control this, we will use a pseudo-random number generator (e.g., Java's Random class) initialized with a specific seed at the start of each test run. By using the same seed, the generator produces the exact same sequence of "random" numbers, making probabilistic events completely repeatable.

* **Asynchronous, Non-Blocking I/O:** All I/O operations (network and storage) are asynchronous and must not block the event loop. We use a custom `ListenableFuture` for this, ensuring the simulation remains deterministic and efficient.  We can not use Java CompletableFuture because it provides a blocking get method and also has methods like supplyAsync which use a separate thread pool.



## Phase 1: Foundational Models & Codecs

### ‚úÖ **COMPLETED** 
- [x] **NetworkAddress** record (`String ipAddress`, `int port`) + port validation (1-65535)
- [x] **MessageType** enum (7 types: CLIENT_GET/SET_REQUEST/RESPONSE, INTERNAL_GET/SET_REQUEST/RESPONSE)  
- [x] **Message** record (`NetworkAddress source/destination`, `MessageType`, `byte[] payload`) + null validation
- [x] **MessageCodec** interface (`encode(Message)`, `decode(byte[])`)
- [x] **JsonMessageCodec** implementation (Jackson-based, simplified without custom serializers)

### ‚úÖ **BONUS: Structured Payload Types**
- [x] **GetRequest** record (`String key`) + validation
- [x] **SetRequest** record (`String key`, `byte[] value`) + validation  
- [x] **GetResponse** record (`String key`, `byte[] value`, `boolean found`) + validation
- [x] **SetResponse** record (`String key`, `boolean success`) + validation
- [x] **MessagePayloadSerializationTest** demonstrating type-safe messaging patterns

### ‚úÖ **RECENTLY COMPLETED**
- [x] **VersionedValue** record (`byte[] value`, `long timestamp`) + null validation & proper byte[] equality
- [x] **Replica** class (basic properties: `String name`, `NetworkAddress`, `List<NetworkAddress> peers`) + tick() method
- [x] **Network** interface with `send()`, `receive()`, `tick()` methods + comprehensive documentation
- [x] **SimulatedNetwork** class with configurable delays, packet loss, deterministic behavior

**Phase 1 & 2 Complete! (2A + 2B + 2C):** 67/67 tests passing ‚úÖ (advanced network simulation with comprehensive refactoring)

---

## Phase 2: Network Layer ‚úÖ **COMPLETED** 

### Phase 2A: Basic Network Implementation ‚úÖ
- [x] **Network** interface (`send()`, `receive()`, `tick()`) with comprehensive documentation
- [x] **SimulatedNetwork** implementation with:
  - [x] Constructor (config, seeded Random, configurable delays and packet loss)
  - [x] Internal packet queue with deterministic delivery timing
  - [x] `send()` method with packet loss simulation using seeded Random
  - [x] `tick()` method with detailed reactive Service Layer documentation
  - [x] Deterministic behavior with identical results for same seeds

### Phase 2B: Advanced Network Simulation ‚úÖ **COMPLETED**
- [x] **Network Partitioning Support**:
  - [x] `partition(NetworkAddress source, NetworkAddress destination)` - bidirectional partition
  - [x] `partitionOneWay(NetworkAddress source, NetworkAddress destination)` - unidirectional partition  
  - [x] `healPartition(NetworkAddress source, NetworkAddress destination)` - restore connectivity
- [x] **Per-Link Configuration**:
  - [x] `setDelay(NetworkAddress source, NetworkAddress destination, int delayTicks)` - specific link delays (implementation complete)
  - [x] `setPacketLoss(NetworkAddress source, NetworkAddress destination, double lossRate)` - per-link packet loss
- [x] **Partition Management**:
  - [x] Track partitioned links in internal state
  - [x] Apply partitions during message sending (drop partitioned messages)
  - [x] Support asymmetric network conditions (A‚ÜíB works, B‚ÜíA blocked)

### Phase 2C: Advanced Refactoring & Code Quality ‚úÖ **COMPLETED**
- [x] **Domain-Driven Design Improvements**:
  - [x] Extracted intention-revealing methods from `send()`: `validateMessage()`, `linkFrom()`, `shouldDropPacket()`, `calculateDeliveryTick()`
  - [x] Enhanced `NetworkLink` record with domain behavior: `isPartitioned()` method
  - [x] Explicit dependencies: `deliverPendingMessagesFor(tickTime)` parameter
- [x] **Performance & Algorithm Enhancements**:
  - [x] Replaced `LinkedList` with `PriorityQueue` for message ordering by delivery time
  - [x] Implemented `Comparable<QueuedMessage>` for automatic priority ordering
  - [x] Improved efficiency from O(n) to O(log n) for message processing
- [x] **Naming & Domain Language**:
  - [x] Renamed `removePartition()` ‚Üí `healPartition()` (correct distributed systems terminology)
  - [x] Renamed `oneWayPartition()` ‚Üí `partitionOneWay()` (natural language flow)
  - [x] Renamed `isPacketLost()` ‚Üí `shouldDropPacket()` (intention-revealing decision method)

### Phase 2D: Production NIO Network Implementation ‚úÖ **COMPLETED**
- [x] **NioNetwork** production-ready implementation with Java NIO:
  - [x] Non-blocking I/O using NIO channels and Selector
  - [x] Multiple server socket binding for different addresses  
  - [x] Deterministic tick() behavior for simulation compatibility
  - [x] Network partitioning support for testing distributed scenarios
  - [x] Thread-safe concurrent access with proper resource management

- [x] **üîß CRITICAL BUG FIXES COMPLETED**:
  - [x] **Message Queuing System**: Messages now properly queued when connections aren't ready (instead of being dropped)
  - [x] **Channel Management**: Fixed multiple channel creation issue by using `isOpen()` instead of `isConnected()` 
  - [x] **Multiple Message Handling**: Fixed TCP message framing to decode all messages in a single buffer (not just the first)
  - [x] **Test Isolation**: Fixed flaky integration tests by improving state cleanup between test runs

- [x] **Production Features**:
  - [x] Async connection establishment with proper state management
  - [x] Partial write handling with selector-based event management  
  - [x] Message queuing for connection-pending scenarios
  - [x] Comprehensive error handling and resource cleanup
  - [x] Debug logging for troubleshooting complex distributed scenarios

- [x] **Integration Testing**:
  - [x] `SimpleNioIntegrationTest`: 3 tests for basic NIO messaging scenarios
  - [x] `ProductionQuorumIntegrationTest`: 7 tests for production quorum with RocksDB storage  
  - [x] **All NIO tests now passing**: Fixed the "1 out of 3 messages" issue and flaky behavior
  - [x] **End-to-End Production Stack**: Full NIO + RocksDB + Quorum consensus working reliably

---

## Phase 3: MessageBus Layer ‚úÖ **COMPLETED**

- [x] **MessageBus** class:
  - [x] Constructor (Network, MessageCodec dependencies)
  - [x] `sendMessage(Message)` method  
  - [x] `registerHandler()/unregisterHandler()` for component registration
  - [x] `tick()` method with automatic message routing to registered handlers
  - [x] `broadcast()` method for multicast messaging
  - [x] Complete message routing and handler management system

---

## Phase 4: Storage Layer ‚úÖ **COMPLETED**

- [x] **Storage** interface (`ListenableFuture<VersionedValue> get()`, `ListenableFuture<Boolean> set()`, `tick()`)
- [x] **BytesKey** record (wraps `byte[]` with proper equals/hashCode for Map keys + defensive copying)
- [x] **SimulatedStorage** implementation:
  - [x] Constructor (fault config, seeded Random)
  - [x] Async `get()`/`set()` methods returning futures
  - [x] `tick()` method completing queued operations + failure simulation
  - [x] PriorityQueue for efficient operation ordering by completion time
  - [x] Configurable delays and failure injection for testing

---

## Phase 5: ListenableFuture Implementation ‚úÖ **COMPLETED**

- [x] **ListenableFuture<T>** class:
  - [x] Single-threaded safe design (no blocking/external threads)
  - [x] States: PENDING, SUCCEEDED, FAILED
  - [x] `complete(T)`, `fail(Throwable)`, `onSuccess(Consumer<T>)` methods
  - [x] `onFailure(Consumer<Throwable>)` method for error handling
  - [x] Multiple callback support with immediate execution if already completed
  - [x] Proper state transition validation and error handling

---

## Phase 6: Replica Quorum Logic ‚úÖ **COMPLETED**

- [x] Enhance **Replica** class:
  - [x] Add Storage reference and quorum tracking (`Map<CorrelationId, QuorumState>`)
  - [x] `onMessageReceived()` router method implementing MessageHandler interface
  - [x] CLIENT_GET_REQUEST & CLIENT_SET_REQUEST handlers (coordinator role)
  - [x] INTERNAL_GET_REQUEST & INTERNAL_SET_REQUEST handlers (participant role) 
  - [x] INTERNAL_GET_RESPONSE & INTERNAL_SET_RESPONSE handlers (coordinator role)
  - [x] `tick()` method for request timeouts and cleanup
  - [x] Correlation ID generation and unique tracking across distributed operations
  - [x] Quorum calculation (majority) and response aggregation logic
  - [x] Timeout handling with configurable timeout ticks

### ‚úÖ **MAJOR ARCHITECTURAL IMPROVEMENT: Clean Message API Separation**
- [x] **Client API Messages**: GetRequest/SetRequest/GetResponse/SetResponse (clean, no internal concerns)
- [x] **Internal API Messages**: Internal* versions with explicit `correlationId` fields for distributed tracking
- [x] **Type Safety**: Prevents mixing client-facing and internal distributed system messages
- [x] **Separation of Concerns**: Client API stays simple while internal API handles distributed complexity
- [x] **19 additional tests** ensuring comprehensive coverage of both API layers

---

## Phase 7: Client Implementation ‚úÖ **COMPLETED**

- [x] **Client** class:
  - [x] Pending requests tracking (`Map<CorrelationId, ListenableFuture>`)
  - [x] `sendGetRequest()` and `sendSetRequest()` methods returning ListenableFuture
  - [x] `onMessageReceived()` response handler implementing MessageHandler interface
  - [x] `tick()` method for request timeouts and cleanup
  - [x] Correlation ID generation for unique request tracking
  - [x] Configurable timeout handling with async future completion
  - [x] Clean client API using GetRequest/SetRequest/GetResponse/SetResponse
  - [x] Request/response matching by key for client-side correlation
  - [x] Comprehensive error handling and timeout management

---

## Phase 8: Advanced Integration Testing ‚úÖ **COMPLETED**

### ‚úÖ **COMPREHENSIVE DISTRIBUTED SYSTEM SCENARIOS**
- [x] **End-to-End Operations**: Full Client ‚Üí Replica ‚Üí Storage ‚Üí Network simulation with quorum consensus
- [x] **Network Partition Tolerance**: Split-brain scenarios, majority/minority partition handling (CAP theorem)
- [x] **Replica Failure Handling**: Graceful degradation, automatic failover, continued operation
- [x] **Concurrent Operations**: Multiple clients, proper serialization, high-throughput scenarios
- [x] **Quorum Requirements**: W+R > N consistency model, prevents split-brain writes
- [x] **Conflict Resolution**: Last-write-wins with timestamps, automatic partition healing
- [x] **Eventual Consistency**: Convergence demonstration, anti-entropy mechanisms
- [x] **Read Repair**: Automatic stale data detection, background synchronization

### ‚úÖ **REAL-WORLD DISTRIBUTED SCENARIOS**
- [x] **`shouldPerformEndToEndGetSetOperation()`**: Basic distributed GET/SET with 5-replica quorum
- [x] **`shouldHandleNetworkPartition()`**: Partition tolerance with 3-vs-2 replica splits
- [x] **`shouldHandleReplicaFailure()`**: Byzantine fault tolerance with replica removal
- [x] **`shouldHandleConcurrentOperations()`**: High-contention concurrent client operations
- [x] **`shouldDemonstrateQuorumRequirements()`**: Quorum consensus validation
- [x] **`shouldHandleConflictResolution()`**: Partition healing and consistency recovery
- [x] **`shouldDemonstrateEventualConsistency()`**: Convergence after network partitions
- [x] **`shouldHandleReadRepairScenario()`**: Read-time consistency repairs

### ‚úÖ **TECHNICAL ACHIEVEMENTS**
- [x] **Enhanced Replica**: Added `storage.tick()` integration for proper storage processing
- [x] **Comprehensive Test Framework**: `DistributedSystemIntegrationTest` with 8 advanced scenarios
- [x] **Deterministic Simulation**: All scenarios reproducible with consistent results
- [x] **Production-Ready Features**: Handles real-world failure scenarios comprehensively

---

## Phase 9: Clean Architecture - Common Building Blocks ‚úÖ **COMPLETED**

### ‚úÖ **ARCHITECTURAL INSIGHT: Separation of Common vs Algorithm-Specific Logic**
**Problem**: The original `Replica` class contained both common building blocks (shared across all replication algorithms) and quorum-specific logic (only relevant to quorum-based consensus).

**Solution**: Extracted common building blocks into a reusable architecture that supports multiple replication algorithms (Raft, Chain Replication, Byzantine Fault Tolerance, etc.).

### ‚úÖ **ARCHITECTURAL REFACTORING COMPLETED**
- [x] **`Replica` (Abstract Base Class)**:
  - [x] Common building blocks: replica identity, request ID generation, timeout management
  - [x] Message routing dispatcher (`onMessageReceived()`)
  - [x] Storage and MessageBus integration
  - [x] Serialization/deserialization utilities
  - [x] Basic `tick()` lifecycle management
  - [x] Abstract `PendingRequest` base class for request tracking

- [x] **`QuorumBasedReplica extends Replica`**:
  - [x] Quorum-specific consensus logic (QuorumState, majority calculation)
  - [x] Coordinator/participant role handling
  - [x] Quorum response aggregation and conflict resolution
  - [x] Timestamp-based ordering for distributed consistency
  - [x] All existing quorum functionality preserved

- [x] **Clean Test Architecture**:
  - [x] `ReplicaBaseTest` - Tests common building blocks (12 tests)
  - [x] `QuorumBasedReplicaTest` - Tests quorum-specific logic (renamed from `EnhancedReplicaTest`)
  - [x] `ReplicaTest` - Tests basic abstract replica functionality

### ‚úÖ **ARCHITECTURAL BENEFITS ACHIEVED**
- [x] **Reusability**: Common building blocks can be shared across different replication algorithms
- [x] **Maintainability**: Clear separation of concerns between infrastructure and algorithm logic
- [x] **Extensibility**: Easy to implement new replication algorithms (Raft, Chain Replication, etc.)
- [x] **Backward Compatibility**: Existing code continues to work without changes
- [x] **Type Safety**: Proper inheritance hierarchy with abstract base classes

---

## Phase 10: Critical Production Architecture Fixes ‚úÖ **COMPLETED**

### ‚úÖ **MAJOR FIX: Client Connection Architecture**
**Problem**: Client was registering against IP addresses upfront instead of establishing connections per request, violating real-world distributed system patterns.

**Solution**: Implemented industry-standard bootstrap discovery pattern inspired by Kafka and TigerBeetle:
- [x] **Bootstrap Replicas Pattern**: Client constructor now accepts `List<NetworkAddress> bootstrapReplicas`
- [x] **Dynamic Connection Establishment**: `establishConnectionAndSend()` method connects per request
- [x] **Network Layer Abstraction**: Added `establishConnection()` method to Network interface
- [x] **SimulatedNetwork Implementation**: Uses localhost with ephemeral ports for deterministic testing
- [x] **NioNetwork Implementation**: Real socket connections with OS-assigned addresses after blocking connect
- [x] **Failover Support**: Client iterates through bootstrap replicas for automatic failover
- [x] **Connection Pooling Ready**: Architecture supports future connection reuse optimizations

### ‚úÖ **CRITICAL FIX: NIONetwork Bidirectional Communication**
**Problem**: NIONetwork was not properly handling client-side vs server-side connections, causing response delivery issues.

**Solution**: Implemented proper channel management following production networking patterns:
- [x] **Accepted Client Channels**: Added `acceptedClientChannels` map to track server-side connections
- [x] **Enhanced handleAccept()**: Properly stores accepted client connections with remote address keys
- [x] **Updated sendMessageDirectly()**: Checks accepted channels first for sending responses back to clients
- [x] **Extended findDestinationForChannel()**: Searches both outbound and accepted channel types
- [x] **Structural vs Behavioral Separation**: Followed TDD and Tidy First principles
- [x] **Test Validation**: All 220+ tests continue to pass after structural changes

### ‚úÖ **KNOWLEDGE ARTIFACT: "Growing a Language" Article**
**Achievement**: Created comprehensive article demonstrating Guy Steele's "Growing a Language" concept using our development session.

**Content Includes**:
- [x] **7 Architectural Layers**: Foundation ‚Üí Network ‚Üí Storage ‚Üí Messaging ‚Üí Consensus ‚Üí Client ‚Üí Testing
- [x] **Language Evolution**: From raw Java networking to domain-specific distributed systems vocabulary
- [x] **AI as Language Growth Catalyst**: Pattern recognition, architectural guidance, refactoring direction
- [x] **Complex Algorithm Enablement**: How grown language enables LLM implementation of Paxos, Raft, PBFT
- [x] **Industry Pattern Integration**: Real-world examples from Kafka, TigerBeetle, distributed systems

### ‚úÖ **ARCHITECTURAL IMPROVEMENTS**
- [x] **Connection Management**: Proper separation of client/server connection lifecycle
- [x] **Bootstrap Discovery**: Industry-standard cluster discovery and failover patterns
- [x] **Network Abstraction**: Clean interface supporting both simulation and production
- [x] **Production Readiness**: Real networking with proper connection establishment
- [x] **Testing Determinism**: Maintained simulation capabilities for reproducible testing

---

## Phase 11: NIONetwork Critical Client-Server Connection Fixes ‚è≥ **‚Üê NEXT**

### üö® **CRITICAL ISSUE: Broken Client-Server Request-Response Flow**
**Problem**: NIONetwork has fundamental architectural flaws in handling client-server communication patterns that cause data corruption and broken request-response flows.

**Root Cause Analysis**:
- [ ] **üî• DATA CORRUPTION**: Shared `readBuffer` across all channels causes data corruption and lost messages
- [ ] **üî• CONNECTION DIRECTION CONFUSION**: `sendMessageDirectly()` has backwards logic for client requests vs server responses
- [ ] **üî• MESSAGE CONTEXT LOSS**: No way to associate responses with the original request channel, breaking server responses
- [ ] **üî• RESOURCE LEAKS**: Incomplete connection cleanup in error scenarios and connection close events
- [ ] **üî• NO REQUEST-RESPONSE CORRELATION**: Missing correlation between inbound requests and outbound responses
- [ ] **üî• BROKEN RESPONSE ROUTING**: Server responses create new outbound connections instead of using existing inbound channels

**Industry Comparison**:
- [ ] **Kafka Pattern**: Per-channel buffers, context-aware message routing, proper request-response correlation
- [ ] **Cassandra Pattern**: Channel state management, connection lifecycle tracking, response routing via original channel
- [ ] **TigerBeetle Pattern**: Minimal connection state, efficient buffer management, clear request/response semantics
- [ ] **YugabyteDB Pattern**: Connection pooling, proper cleanup, message correlation tracking

### üîß **Critical Fixes Required (Phase 1: Data Corruption Prevention)**
- [ ] **Fix 1: Per-Channel Buffer Management**:
  - [ ] Replace shared `readBuffer`/`writeBuffer` with per-channel buffers
  - [ ] `Map<SocketChannel, ChannelState>` with individual read/write buffers
  - [ ] Prevent data corruption from concurrent channel access
- [ ] **Fix 2: Message Context Preservation**:
  - [ ] Create `MessageContext` class to track source channel and message metadata
  - [ ] Modify `handleRead()` to preserve channel context with decoded messages
  - [ ] Enable proper response routing back to original channel

### üîß **Critical Fixes Required (Phase 2: Connection Flow Correction)**
- [ ] **Fix 3: Request-Response Correlation**:
  - [ ] Add correlation ID tracking between requests and responses
  - [ ] Map correlation IDs to source channels for response routing
  - [ ] Implement context-aware message routing logic
- [ ] **Fix 4: Connection Direction Logic**:
  - [ ] Fix `sendMessageDirectly()` to distinguish client requests vs server responses
  - [ ] Route responses via original inbound channel, not new outbound connections
  - [ ] Separate outbound (client‚Üíserver) and response (server‚Üíclient) flows

### üîß **Critical Fixes Required (Phase 3: Resource Management)**
- [ ] **Fix 5: Connection Cleanup**:
  - [ ] Comprehensive cleanup in `handleRead()` when connection closes
  - [ ] Remove channels from all tracking maps and clean up resources
  - [ ] Implement proper connection lifecycle management
- [ ] **Fix 6: Error Handling**:
  - [ ] Add proper error handling in all connection operations
  - [ ] Implement connection recovery and retry logic
  - [ ] Add connection health monitoring and timeout detection

### üéØ **Implementation Strategy**
- [ ] **Phase 1: Critical Data Corruption Fixes** (Immediate - prevents data loss):
  - [ ] Create `ChannelState` class for per-channel buffer management
  - [ ] Replace shared buffers with per-channel buffers in `handleRead()/handleWrite()`
  - [ ] Create `MessageContext` class to preserve source channel information
  - [ ] Validate all tests still pass with structural changes
- [ ] **Phase 2: Request-Response Flow Fixes** (High Priority - enables proper communication):
  - [ ] Add correlation tracking infrastructure (`correlationId ‚Üí SocketChannel` mapping)
  - [ ] Fix `sendMessageDirectly()` routing logic for requests vs responses
  - [ ] Implement context-aware message routing in `processOutboundMessages()`
  - [ ] Add response routing via original inbound channels
- [ ] **Phase 3: Resource Management & Cleanup** (Medium Priority - prevents leaks):
  - [ ] Comprehensive connection cleanup in error scenarios
  - [ ] Proper channel removal from all tracking maps
  - [ ] Connection lifecycle state management
  - [ ] Enhanced error handling and recovery mechanisms
- [ ] **Phase 4: Production Validation & Testing**:
  - [ ] All existing tests must continue to pass
  - [ ] Add specific tests for client-server request-response flows
  - [ ] Performance testing for buffer management overhead
  - [ ] Validate against production distributed system patterns

---

## Phase 12: Simulation Driver ‚è≥ **FUTURE**

- [ ] **SimulationDriver** class:
  - [ ] `main()` method with complete setup
  - [ ] Dependency injection (Random, Network, Storage, MessageBus, Replicas, Clients)
  - [ ] Simulation loop with correct tick() ordering

---

## Current Implementation Status

### üìÅ **Project Structure**
```
src/main/java/replicated/
‚îú‚îÄ‚îÄ messaging/
‚îÇ   ‚îú‚îÄ‚îÄ NetworkAddress.java         ‚úÖ (with port validation)
‚îÇ   ‚îú‚îÄ‚îÄ MessageType.java           ‚úÖ (7 message types)  
‚îÇ   ‚îú‚îÄ‚îÄ Message.java              ‚úÖ (with null validation & proper equals)
‚îÇ   ‚îú‚îÄ‚îÄ MessageCodec.java         ‚úÖ (interface)
‚îÇ   ‚îú‚îÄ‚îÄ JsonMessageCodec.java     ‚úÖ (simplified, no custom serializers)
‚îÇ   ‚îú‚îÄ‚îÄ MessageHandler.java       ‚úÖ (interface for message recipients)
‚îÇ   ‚îú‚îÄ‚îÄ MessageBus.java           ‚úÖ (higher-level messaging with routing & broadcast)
‚îÇ   ‚îú‚îÄ‚îÄ GetRequest.java           ‚úÖ (client request)
‚îÇ   ‚îú‚îÄ‚îÄ SetRequest.java           ‚úÖ (client request)  
‚îÇ   ‚îú‚îÄ‚îÄ GetResponse.java          ‚úÖ (server response)
‚îÇ   ‚îú‚îÄ‚îÄ SetResponse.java          ‚úÖ (server response)
‚îÇ   ‚îú‚îÄ‚îÄ InternalGetRequest.java   ‚úÖ (internal distributed request)
‚îÇ   ‚îú‚îÄ‚îÄ InternalSetRequest.java   ‚úÖ (internal distributed request)
‚îÇ   ‚îú‚îÄ‚îÄ InternalGetResponse.java  ‚úÖ (internal distributed response)
‚îÇ   ‚îî‚îÄ‚îÄ InternalSetResponse.java  ‚úÖ (internal distributed response)
‚îú‚îÄ‚îÄ network/
‚îÇ   ‚îú‚îÄ‚îÄ Network.java              ‚úÖ (interface with comprehensive documentation + partitioning methods)
‚îÇ   ‚îú‚îÄ‚îÄ SimulatedNetwork.java     ‚úÖ (PriorityQueue, domain-driven refactoring, partitioning, per-link config)
‚îÇ   ‚îî‚îÄ‚îÄ NioNetwork.java           ‚úÖ (production NIO implementation with bug fixes)
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ VersionedValue.java       ‚úÖ (value + timestamp with proper byte[] equality)
‚îÇ   ‚îú‚îÄ‚îÄ Storage.java              ‚úÖ (async interface with ListenableFuture)
‚îÇ   ‚îú‚îÄ‚îÄ BytesKey.java             ‚úÖ (byte[] wrapper with defensive copying + proper Map key behavior)
‚îÇ   ‚îú‚îÄ‚îÄ SimulatedStorage.java     ‚úÖ (async storage with delays, failures, PriorityQueue)
‚îÇ   ‚îî‚îÄ‚îÄ RocksDbStorage.java       ‚úÖ (production persistent storage with RocksDB)
‚îú‚îÄ‚îÄ future/
‚îÇ   ‚îî‚îÄ‚îÄ ListenableFuture.java     ‚úÖ (single-threaded async with callbacks, states, multiple handlers)
‚îú‚îÄ‚îÄ replica/
‚îÇ   ‚îú‚îÄ‚îÄ Replica.java              ‚úÖ (abstract base class with common building blocks)
‚îÇ   ‚îî‚îÄ‚îÄ QuorumBasedReplica.java   ‚úÖ (quorum-specific consensus extending Replica)
‚îî‚îÄ‚îÄ client/
    ‚îî‚îÄ‚îÄ Client.java               ‚úÖ (bootstrap discovery, async requests, response handling, timeout management, correlation tracking)

src/test/java/replicated/integration/
‚îú‚îÄ‚îÄ DistributedSystemIntegrationTest.java ‚úÖ (comprehensive real-world distributed scenarios)
‚îú‚îÄ‚îÄ SimpleNioIntegrationTest.java         ‚úÖ (basic NIO network testing)
‚îî‚îÄ‚îÄ ProductionQuorumIntegrationTest.java  ‚úÖ (full production stack: NIO + RocksDB + Quorum)
```

### üß™ **Test Coverage: 220+ Passing**
- NetworkAddress: 6 tests (creation, equality, port validation)
- MessageType: 1 test (enum completeness)
- Message: 6 tests (creation, equality, null validation)  
- MessageCodec: 8 tests (encoding, decoding, error handling)
- **Client API Messages:**
  - GetRequest: 3 tests (clean client API, no correlation IDs)
  - SetRequest: 4 tests (clean client API, no internal fields)
  - GetResponse: 5 tests (VersionedValue support, clean client responses)  
  - SetResponse: 3 tests (simple success/failure responses)
- **Internal API Messages:**
  - InternalGetRequest: 4 tests (correlation ID tracking)
  - InternalSetRequest: 5 tests (correlation ID + timestamp)
  - InternalGetResponse: 5 tests (correlation ID + VersionedValue)
  - InternalSetResponse: 5 tests (correlation ID + success tracking)
- MessagePayloadSerialization: 4 tests (type-safe messaging patterns)
- **MessageBus: 14 tests (component registration, message routing, broadcast, handler management, tick coordination)**
- **ListenableFuture: 20 tests (states, callbacks, error handling, multiple handlers, validation)**
- **Storage & BytesKey: 21 tests (async operations, defensive copying, Map key behavior, fault injection)**
- VersionedValue: 8 tests (creation, equality, validation, byte[] handling)
- **Replica Architecture Tests:**
  - ReplicaBaseTest: 12 tests (common building blocks, timeout handling, request ID generation)
  - QuorumBasedReplicaTest: 13 tests (quorum logic, distributed consensus, message routing)
  - ReplicaTest: 7 tests (basic abstract replica functionality)
- **Client: 14 tests (async requests, response handling, timeout management, correlation tracking, clean API)**
- **SimulatedNetwork: 14 tests (send/receive, delays, packet loss, deterministic behavior, partitioning, per-link config)**
  - **ENHANCED**: Advanced network simulation with partitioning, asymmetric conditions, and domain-driven refactoring
  - **PERFORMANCE**: Upgraded to PriorityQueue for O(log n) message processing efficiency
- **üÜï DistributedSystemIntegration: 8 tests (comprehensive real-world scenarios)**
  - **End-to-End Operations**: Full distributed GET/SET with 5-replica quorum
  - **Network Partitions**: Split-brain scenarios, majority/minority handling  
  - **Replica Failures**: Byzantine fault tolerance, automatic failover
  - **Concurrent Operations**: High-contention multi-client scenarios
  - **Quorum Requirements**: W+R > N consistency model validation
  - **Conflict Resolution**: Partition healing, consistency recovery
  - **Eventual Consistency**: Convergence demonstration, anti-entropy
  - **Read Repair**: Stale data detection, background synchronization

- **üÜï Production NIO Network: 12 tests (production-ready networking)**
  - **NioNetworkTest**: Socket management, connection handling, message transmission
  - **SimpleNioIntegrationTest**: End-to-end messaging scenarios with real networking
  - **Multiple Message Handling**: TCP framing, message queuing, connection establishment

- **üÜï Production RocksDB Storage: 8 tests (persistent storage)**
  - **RocksDbStorageTest**: Database operations, key-value persistence, async behavior
  - **ProductionQuorumIntegrationTest**: Full production stack with NIO + RocksDB + Quorum consensus

### üöÄ **Current Status: Complete Production-Ready Distributed System**
- **Phases 1-10 Complete**: Full distributed system with clean architecture, extensible replica framework, and critical production fixes
- **Dual Implementation Strategy**: Both simulation (deterministic testing) and production (real networking) versions
- **Production Stack**: NIO networking + RocksDB persistence + Quorum consensus working reliably
- **Comprehensive Bug Fixes**: Message queuing, TCP framing, test isolation, client connection architecture, NIO bidirectional communication all resolved
- **Advanced Features**: Network partitions, replica failures, quorum consensus, conflict resolution, bootstrap discovery, failover support
- **Deterministic Testing**: All scenarios reproducible with consistent results
- **Industry Standards**: Kafka/TigerBeetle-inspired connection patterns, proper client/server channel management
- **Architecture Ready**: Easy to implement Raft, Chain Replication, Byzantine Fault Tolerance, etc.
- **Knowledge Artifact**: "Growing a Language" article documenting the development methodology
- **Next**: Phase 11 - Simulation Driver for complete system orchestration

---

## Development Methodology

Following **TDD cycle**: Red ‚Üí Green ‚Üí Refactor  
Following **Tidy First**: Structural changes separate from behavioral changes  
**Commit discipline**: Only when all tests pass, clear behavioral vs structural messages  
**Code quality**: Eliminate duplication, express intent clearly, simplest solution that works 